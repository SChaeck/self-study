{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **요약**\n",
    "* **간단한 답변**\n",
    "    1. LLM 설정\n",
    "    2. LLM.invoke(question)\n",
    "    \n",
    "* **LLM Chain을 사용한 답변**\n",
    "    1. LLM 설정\n",
    "    2. 문자열로 template 작성 ({}를 사용해 input_variables 지정)\n",
    "    3. template으로 PromptTemplate 생성\n",
    "    4. LLMChain(prompt=prompt, llm=llm) 생성\n",
    "    5. 답변 받아오기\n",
    "        a. invoke(): 단일 질문\n",
    "        b. apply(): 복수 질문 (리스트 형태)\n",
    "        c. generate(): 복수 질문 + 메타 데이터\n",
    "\n",
    "* **streaming** \n",
    "    1. 답변을 받는 방법은 위와 동일하나 LLM 설정 단계가 다름\n",
    "        a. streaming=True\n",
    "        b. callbacks=[StreamingStdOutCallbackHandler()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[답변]: content='서울입니다.' response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 27, 'total_tokens': 32}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    max_tokens=2048,\n",
    "    model_name='gpt-3.5-turbo',\n",
    ")\n",
    "\n",
    "question = \"대한민국의 수도는? (짦게 답변)\"\n",
    "\n",
    "print(f\"[답변]: {llm.invoke(question)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프롬프트 템플릿의 활용\n",
    "**PromptTemplate**\n",
    "* 사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다\n",
    "* 사용법\n",
    "    * template: 템플릿 문자열입니다. 이 문자열 내에서 중괄호 {}는 변수를 나타냅니다.\n",
    "    * input_variables: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.\n",
    "\n",
    "**input_variables**\n",
    "* input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.\n",
    "* 사용법: 리스트 형식으로 변수 이름을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], template='{country}의 수도는 뭐야?')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"{country}의 수도는 뭐야?\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain 객체\n",
    "**LLMChain**\n",
    "* LLMChain은 특정 <U>PromptTemplate</U>와 연결된 체인 객체를 생성합니다\n",
    "* 사용법\n",
    "    * prompt: 앞서 정의한 PromptTemplate 객체를 사용합니다.\n",
    "    * llm: 언어 모델을 나타내며, 이 예시에서는 이미 어딘가에서 정의된 것으로 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': '대한민국', 'text': '대한민국의 수도는 서울이야.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"country\": \"대한민국\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': '캐나다', 'text': '캐나다의 수도는 오타와(Ottawa)입니다.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"country\": \"캐나다\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**apply()**\n",
    "* 여러개의 입력에 대한 처리를 한 번에 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [{\"country\": \"호주\"}, {\"country\": \"중국\"}, {\"country\": \"네덜란드\"}]\n",
    "\n",
    "response = llm_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "호주의 수도는 캔버라입니다.\n",
      "중국의 수도는 베이징(北京)입니다.\n",
      "네덜란드의 수도는 암스테르담입니다.\n"
     ]
    }
   ],
   "source": [
    "for res in response:\n",
    "    print(res['text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate()**\n",
    "* LLMResult를 반환하는 점을 제외하고는 apply와 유사하다.\n",
    "* 토큰사용량과 종료 이유와 같은 유용한 생성 정보를 포함한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text='호주의 수도는 캔버라입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='호주의 수도는 캔버라입니다.', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 17, 'total_tokens': 30}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}))], [ChatGeneration(text='중국의 수도는 베이징(北京)입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='중국의 수도는 베이징(北京)입니다.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 19, 'total_tokens': 37}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}))], [ChatGeneration(text='네덜란드의 수도는 암스테르담입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='네덜란드의 수도는 암스테르담입니다.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 22, 'total_tokens': 44}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}))]] llm_output={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 58, 'total_tokens': 111}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c'} run=[RunInfo(run_id=UUID('a18a24c4-e89b-4331-b404-7ea90fe2b8af')), RunInfo(run_id=UUID('e2bf8c05-a82c-4907-ac22-70f41c55a87a')), RunInfo(run_id=UUID('e83d9a96-1fa5-413b-9901-a73e076767ca'))]\n"
     ]
    }
   ],
   "source": [
    "generated_result = llm_chain.generate(input_list)\n",
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 53,\n",
       "  'prompt_tokens': 58,\n",
       "  'total_tokens': 111},\n",
       " 'model_name': 'gpt-3.5-turbo',\n",
       " 'system_fingerprint': 'fp_3bc1b5746c'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_variables 여러 개 사용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['area1', 'area2'], template='{area1} 와 {area2} 의 시차는 몇 시간이야?')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"{area1} 와 {area2} 의 시차는 몇 시간이야?\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['area1', 'area2'], template='{area1} 와 {area2} 의 시차는 몇 시간이야?'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000232DD01C520>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000232DD02E130>, temperature=0.1, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=2048))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'area1': '서울', 'area2': '파리', 'text': '서울과 파리의 시차는 8시간입니다. 서울은 GMT+9 시간대에 속하고, 파리는 GMT+1 시간대에 속하기 때문에 시차가 8시간이 발생합니다.'}\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.invoke({\"area1\": \"서울\", \"area2\": \"파리\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파리와 뉴욕의 시차는 6시간입니다. 파리는 그리니치 평균시(GMT+1)를 따르고, 뉴욕은 동부 표준시(EST 또는 GMT-5)를 따르기 때문에 시차가 발생합니다.\n",
      "서울과 하와이의 시차는 서울이 하와이보다 19시간 빠릅니다. 서울은 GMT+9 시간대에 위치하고 있으며, 하와이는 GMT-10 시간대에 위치하고 있기 때문입니다.\n",
      "켄버라와 베이징의 시차는 2시간입니다. 켄버라는 UTC+10 시간대에 있고, 베이징은 UTC+8 시간대에 있기 때문입니다.\n"
     ]
    }
   ],
   "source": [
    "input_list = [\n",
    "    {\"area1\": \"파리\", \"area2\": \"뉴욕\"},\n",
    "    {\"area1\": \"서울\", \"area2\": \"하와이\"},\n",
    "    {\"area1\": \"켄버라\", \"area2\": \"베이징\"},\n",
    "]\n",
    "\n",
    "result = llm_chain.apply(input_list)\n",
    "for res in result:\n",
    "    print(res[\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stream: 실시간 출력\n",
    "* 스트리밍 옵션은 질의에 대한 답변을 실시간으로 받을 때 유용하다.\n",
    "* 다음과 같이 **streaming=True**로 설정하고 스트리밍으로 답변을 받기 위한\n",
    "**StreamingStdOutCallbackHandler()** 을 콜백으로 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한반도 동쪽에 위치한 나라로, 수도는 서울이며 한국어를 사용하고 있습니다. 현대화된 경제와 문화를 갖추고 있습니다."
     ]
    }
   ],
   "source": [
    "question = \"대한민국에 대해서 간략하게 20자 내외로 알려줘\"\n",
    "\n",
    "response = llm.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
